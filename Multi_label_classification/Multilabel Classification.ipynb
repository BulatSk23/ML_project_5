{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import time\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from statistics import mean \n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as torch_data\n",
    "import mca\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _loadTxt(fName):\n",
    "    return np.loadtxt(fName, delimiter=',', skiprows=1, dtype=np.float32)\n",
    "\n",
    "def cache(dirPath):\n",
    "    cacheF = os.path.join(dirPath, 'cache.pkl')\n",
    "    if os.path.isfile(cacheF):\n",
    "        with open(cacheF, 'rb') as f:\n",
    "            return pkl.load(f)\n",
    "    return None\n",
    "\n",
    "def loadBibtex(dirPath):\n",
    "    cacheF = os.path.join(dirPath, 'cache.pkl')\n",
    "    d = cache(dirPath)\n",
    "    if d is None:\n",
    "        train = _loadTxt(os.path.join(dirPath, \"bibtex-train.csv\"))\n",
    "        trainX, trainY = np.split(train, [1836], axis=1)\n",
    "        test = _loadTxt(os.path.join(dirPath, \"bibtex-test.csv\"))\n",
    "        testX, testY = np.split(test, [1836], axis=1)\n",
    "        d = {'trainX': trainX, 'trainY': trainY,\n",
    "            'testX': testX, 'testY': testY}\n",
    "        with open(cacheF, 'wb') as f:\n",
    "            pkl.dump(d, f)\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadBibtex(\"bibtex\")\n",
    "\n",
    "trainX = data['trainX']\n",
    "trainY = data['trainY']\n",
    "testX = data['testX']\n",
    "testY = data['testY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4880, 1836), (2515, 1836), (4880, 159), (2515, 159))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape, testX.shape, trainY.shape, testY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_tstX = np.vstack((trainX,testX))\n",
    "tr_tstY = np.vstack((trainY,testY))\n",
    "tr_tstX_data = pd.DataFrame(tr_tstX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run MCA on selected columns of a pd DataFrame.\n",
      "\t\n",
      "\tIf the column are specified, assume that they hold\n",
      "\tcategorical variables that need to be replaced with\n",
      "\tdummy indicators, otherwise process the DataFrame as is.\n",
      "\n",
      "\t'cols': The columns of the DataFrame to process.\n",
      "\t'ncols': The number of columns before dummy coding. To be passed if cols isn't.\n",
      "\t'benzecri': Perform BenzÃ©cri correction (default: True)\n",
      "\t'TOL': value below which to round eigenvalues to zero (default: 1e-4)\n",
      "\t\n"
     ]
    }
   ],
   "source": [
    "mca_ben = mca.MCA(tr_tstX_data, ncols=420)\n",
    "print(mca.MCA.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x = mca_ben.fs_r(1)\n",
    "trainX, testX = np.split(new_x, [4880], axis=0)\n",
    "trainY, testY = np.split(tr_tstY, [4880], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4880, 785), (2515, 785), (4880, 159), (2515, 159))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape, testX.shape, trainY.shape, testY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = trainX.shape[1]\n",
    "output_dim = trainY.shape[1]\n",
    "batch_size_train = trainX.shape[0]//15\n",
    "batch_size_test = testX.shape[0]//10 \n",
    "\n",
    "hidden_layer_sizes=[135, 110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BibtexData(torch_data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super(BibtexData, self).__init__()\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return  self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset = BibtexData(trainX, trainY) \n",
    "test_dset = BibtexData(testX, testY) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseICNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim,  hidden_layer_sizes, activation='celu', dropout=0.3):\n",
    "        super(DenseICNN, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.droput = dropout\n",
    "        self.activation = activation\n",
    "\n",
    "        \n",
    "        self.quadratic_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_dim, output_features, bias=True),\n",
    "                nn.Dropout(dropout))\n",
    "            for output_features in hidden_layer_sizes])\n",
    "        \n",
    "        sizes = zip(hidden_layer_sizes[:-1], hidden_layer_sizes[1:])\n",
    "        self.convex_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_features, output_features, bias=True),\n",
    "                nn.Dropout(dropout))\n",
    "            for (input_features, output_features) in sizes])\n",
    "        \n",
    "        self.final_layer = nn.Linear(hidden_layer_sizes[-1], output_dim, bias=True)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = self.quadratic_layers[0](input)\n",
    "        for quadratic_layer, convex_layer in zip(self.quadratic_layers[1:], self.convex_layers):\n",
    "            output = convex_layer(output) + quadratic_layer(input)\n",
    "            if self.activation == 'celu':\n",
    "                output = torch.celu(output)\n",
    "        return self.final_layer(output)\n",
    "    \n",
    "    def convexify(self):\n",
    "        for layer in self.convex_layers:\n",
    "            for sublayer in layer:\n",
    "                if (isinstance(sublayer, nn.Linear)):\n",
    "                    sublayer.weight.data.clamp_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICNN = DenseICNN(input_dim, output_dim,  hidden_layer_sizes, activation='celu') \n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(ICNN.parameters(), lr=0.006, weight_decay=1e-7) \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.8)\n",
    "\n",
    "\n",
    "train_loader = torch_data.DataLoader(train_dset, batch_size=batch_size_train, shuffle=True) \n",
    "test_loader = torch_data.DataLoader(test_dset, batch_size=batch_size_test, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, ICNN, criterion, optimizer, train_loader, test_loader, scheduler=None, verbose=True, save_dir=None):\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        f1 = []\n",
    "        ICNN.train()\n",
    "        for X, y in train_loader:\n",
    "           \n",
    "            y_out = ICNN(X)\n",
    "\n",
    "            loss = criterion(y, torch.sigmoid(y_out))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            ICNN.convexify()  \n",
    "            \n",
    "        ICNN.eval()\n",
    "        for X, y in test_loader:\n",
    "            \n",
    "            y_out = ICNN(X)\n",
    "\n",
    "            test_loss = criterion(y, torch.sigmoid(y_out))\n",
    "            predY_bin = (torch.sigmoid(y_out).detach().numpy() >= 0.5).astype(np.int)\n",
    "            trueY_bin = y.detach().numpy()\n",
    "            f1.append(f1_score(trueY_bin.T, predY_bin.T, average='macro', pos_label=None))\n",
    "            \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        freq = max(epochs//20,50)\n",
    "        if verbose and epoch%freq==0:\n",
    "            print('Epoch {}/{} || Loss:  Train {:.4f} | Validation {:.4f}'.format(epoch, epochs, loss.item(), test_loss.item()))\n",
    "            print('F1 score:', mean(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500 || Loss:  Train 0.0180 | Validation 0.0063\n",
      "F1 score: 0.14405609883697534\n",
      "Epoch 100/500 || Loss:  Train 0.0149 | Validation 0.0049\n",
      "F1 score: 0.22667493478793732\n",
      "Epoch 150/500 || Loss:  Train 0.0075 | Validation 0.0056\n",
      "F1 score: 0.2776935605480138\n",
      "Epoch 200/500 || Loss:  Train 0.0037 | Validation 0.0040\n",
      "F1 score: 0.3472302635140728\n",
      "Epoch 250/500 || Loss:  Train 0.0082 | Validation 0.0040\n",
      "F1 score: 0.36181467950325535\n",
      "Epoch 300/500 || Loss:  Train 0.0046 | Validation 0.0046\n",
      "F1 score: 0.3661285887672208\n",
      "Epoch 350/500 || Loss:  Train 0.0106 | Validation 0.0045\n",
      "F1 score: 0.36786566213990207\n",
      "Epoch 400/500 || Loss:  Train 0.0041 | Validation 0.0052\n",
      "F1 score: 0.37227686786103836\n",
      "Epoch 450/500 || Loss:  Train 0.0096 | Validation 0.0047\n",
      "F1 score: 0.38239835938270456\n",
      "Epoch 500/500 || Loss:  Train 0.0059 | Validation 0.0044\n",
      "F1 score: 0.3823312916466281\n"
     ]
    }
   ],
   "source": [
    "train(500, ICNN, criterion, optimizer, train_loader, test_loader, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
